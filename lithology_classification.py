
"""–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –ª–∏—Ç–æ–ª–æ–≥–∏–∏ –ø–æ –¥–∞–Ω–Ω—ã–º –ì–ò–°

–ú–æ–¥–µ–ª—å –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏–ø–∞ –≥–æ—Ä–Ω–æ–π –ø–æ—Ä–æ–¥—ã
–ø–æ –¥–∞–Ω–Ω—ã–º –≥–µ–æ—Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ö –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π —Å–∫–≤–∞–∂–∏–Ω.
"""

# =============================================================================
# –ò–ú–ü–û–†–¢ –ë–ò–ë–õ–ò–û–¢–ï–ö
# =============================================================================
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
import zipfile
import io
import warnings
import shap
import os
import sys

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import (accuracy_score, f1_score,
                           classification_report, confusion_matrix)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
warnings.filterwarnings('ignore')

# =============================================================================
# –°–û–ó–î–ê–ù–ò–ï –ü–ê–ü–û–ö –î–õ–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# =============================================================================
print("üìÅ –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø–∞–ø–æ–∫...")

# –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–∞–ø–∫–∏
folders = ['images', 'results', 'data']
for folder in folders:
    if not os.path.exists(folder):
        os.makedirs(folder)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞: {folder}/")
    else:
        print(f"üìÅ –ü–∞–ø–∫–∞ —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {folder}/")

# =============================================================================
# –ó–ê–ì–†–£–ó–ö–ê –ò –ü–†–ï–î–û–ë–†–ê–ë–û–¢–ö–ê –î–ê–ù–ù–´–•
# =============================================================================
print("\nüîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")

try:
    # –°–∫–∞—á–∏–≤–∞–µ–º –∏ —Ä–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º –Ω–∞–ø—Ä—è–º—É—é –≤ –ø–∞–º—è—Ç—å
    zip_url = "https://github.com/bolgebrygg/Force-2020-Machine-Learning-competition/raw/master/lithology_competition/data/train.zip"
    r = requests.get(zip_url)
    z = zipfile.ZipFile(io.BytesIO(r.content))

    # –ß–∏—Ç–∞–µ–º CSV –∏–∑ –∞—Ä—Ö–∏–≤–∞
    df = pd.read_csv(z.open('train.csv'), sep=';')
    print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
except Exception as e:
    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
    sys.exit(1)

# –ü–µ—Ä–µ–≤–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ —Ä—É—Å—Å–∫–∏–π
russian_columns = {
    'WELL': '–°–ö–í–ê–ñ–ò–ù–ê',
    'DEPTH_MD': '–ì–õ–£–ë–ò–ù–ê_–ò–ó–ú–ï–†–ï–ù–ù–ê–Ø',
    'X_LOC': '–ö–û–û–†–î–ò–ù–ê–¢–ê_X',
    'Y_LOC': '–ö–û–û–†–î–ò–ù–ê–¢–ê_Y',
    'Z_LOC': '–ö–û–û–†–î–ò–ù–ê–¢–ê_Z',
    'GROUP': '–ì–†–£–ü–ü–ê',
    'FORMATION': '–§–û–†–ú–ê–¶–ò–Ø',
    'CALI': '–ö–ê–õ–ò–ë–†',
    'RSHA': 'RES_–ó–û–ù–ê_–í–¢–û–†–ñ–ï–ù–ò–Ø',
    'RMED': 'RES_–°–†–ï–î–ù–Ø–Ø',
    'RDEP': 'RES_–ì–õ–£–ë–û–ö–ê–Ø',
    'RHOB': '–ü–õ–û–¢–ù–û–°–¢–¨',
    'GR': '–ì–ö',
    'SGR': '–ì–ö_–°–ü–ï–ö–¢–†–ê–õ–¨–ù–´–ô',
    'NPHI': '–ù–ù–ö',
    'PEF': 'PEF',
    'DTC': 'DTC',
    'SP': '–°–ü',
    'BS': '–†–ê–ó–ú–ï–†_–î–û–õ–û–¢–ê',
    'ROP': '–°–ö–û–†–û–°–¢–¨_–ü–†–û–•–û–î–ö–ò',
    'DTS': 'DTS',
    'DCAL': '–ö–ê–õ–ò–ë–†_–†–ê–°–•–û–ñ–î–ï–ù–ò–ï',
    'DRHO': '–ü–õ–û–¢–ù–û–°–¢–¨_–ü–û–ü–†–ê–í–ö–ê',
    'MUDWEIGHT': '–ü–õ–û–¢–ù–û–°–¢–¨_–†–ê–°–¢–í–û–†–ê',
    'RMIC': 'RES_–ú–ò–ö–†–û',
    'ROPA': 'RES_????',
    'RXO': 'RES_–ó–û–ù–ê_–ü–†–û–ù–ò–ö–ù–û–í–ï–ù–ò–Ø',
    'FORCE_2020_LITHOFACIES_LITHOLOGY': '–õ–ò–¢–û–õ–û–ì–ò–Ø',
    'FORCE_2020_LITHOFACIES_CONFIDENCE': '–î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨_–õ–ò–¢–û–õ–û–ì–ò–ò'
}

df = df.rename(columns=russian_columns)
target = '–õ–ò–¢–û–õ–û–ì–ò–Ø'
df = df.drop('–î–û–°–¢–û–í–ï–†–ù–û–°–¢–¨_–õ–ò–¢–û–õ–û–ì–ò–ò', axis=1)

# =============================================================================
# EDA - –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•
# =============================================================================
print("\nüìä –ë–ê–ó–û–í–´–ô –ê–ù–ê–õ–ò–ó –î–ê–ù–ù–´–•")
print(f"–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–∫–≤–∞–∂–∏–Ω: {df['–°–ö–í–ê–ñ–ò–ù–ê'].nunique()}")
print(f"–î–∏–∞–ø–∞–∑–æ–Ω –≥–ª—É–±–∏–Ω: {df['–ì–õ–£–ë–ò–ù–ê_–ò–ó–ú–ï–†–ï–ù–ù–ê–Ø'].min():.1f} - {df['–ì–õ–£–ë–ò–ù–ê_–ò–ó–ú–ï–†–ï–ù–ù–ê–Ø'].max():.1f} –º")

# –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
print("\nüîç –ê–ù–ê–õ–ò–ó –ü–†–û–ü–£–©–ï–ù–ù–´–• –ó–ù–ê–ß–ï–ù–ò–ô")
miss_data = df.isnull().sum()
miss_per = (miss_data/len(df))*100
miss_info = pd.DataFrame({
    '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ': miss_data,
    '–ü—Ä–æ—Ü–µ–Ω—Ç': miss_per
}).sort_values('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ', ascending=False)
miss_info = miss_info[miss_info['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ'] > 0]
print(miss_info)

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
miss_info.to_csv('results/missing_values_analysis.csv', encoding='utf-8-sig')
print("‚úÖ –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ results/missing_values_analysis.csv")

# –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >80% –ø—Ä–æ–ø—É—Å–∫–æ–≤
cols_todrop = miss_info[miss_info['–ü—Ä–æ—Ü–µ–Ω—Ç'] > 80].index.tolist()
if cols_todrop:
    print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å >80% –ø—Ä–æ–ø—É—Å–∫–æ–≤: {cols_todrop}")
    df = df.drop(cols_todrop, axis=1)
else:
    print("‚úÖ –ù–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å >80% –ø—Ä–æ–ø—É—Å–∫–æ–≤")

# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
features_tofill = ['–ù–ù–ö', '–ü–õ–û–¢–ù–û–°–¢–¨', '–ö–ê–õ–ò–ë–†', 'RES_–ì–õ–£–ë–û–ö–ê–Ø', 'DTC']
print(f"\nüîÑ –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_tofill}")

for f in features_tofill:
    if f in df.columns:
        df[f] = df.groupby('–°–ö–í–ê–ñ–ò–ù–ê')[f].transform(lambda x: x.fillna(x.median()))
        print(f"‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –ø—Ä–æ–ø—É—Å–∫–∏ –¥–ª—è {f}")

if '–ö–ê–õ–ò–ë–†' in df.columns:
    df['–ö–ê–õ–ò–ë–†'] = df['–ö–ê–õ–ò–ë–†'].fillna(df['–ö–ê–õ–ò–ë–†'].median())
    print("‚úÖ –ó–∞–ø–æ–ª–Ω–µ–Ω—ã –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –ö–ê–õ–ò–ë–†")

# =============================================================================
# –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–Ø –ü–†–ò–ó–ù–ê–ö–û–í
# =============================================================================
print("\nüìà –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –î–ê–ù–ù–´–•...")

# –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –æ—Å–Ω–æ–≤–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
main_features = ['–ì–ö', '–ü–õ–û–¢–ù–û–°–¢–¨', 'RES_–ì–õ–£–ë–û–ö–ê–Ø', '–ù–ù–ö', 'DTC', '–ö–ê–õ–ò–ë–†']

for i, feature in enumerate(main_features):
    if feature in df.columns:
        ax = axes[i//3, i%3]
        df[feature].hist(bins=50, ax=ax, alpha=0.7)
        ax.set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {feature}')
        ax.set_xlabel(feature)
        ax.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')

plt.tight_layout()
plt.savefig('images/feature_distributions.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úÖ –ì—Ä–∞—Ñ–∏–∫–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ images/feature_distributions.png")

# =============================================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –¶–ï–õ–ï–í–û–ô –ü–ï–†–ï–ú–ï–ù–ù–û–ô
# =============================================================================
lithology_dict = {
    65000: '–ü–µ—Å—á–∞–Ω–∏–∫',
    30000: '–ü–µ—Å—á–∞–Ω–∏–∫-–ì–ª–∏–Ω–∞',
    65030: '–ì–ª–∏–Ω–∏—Å—Ç—ã–π –ø–µ—Å—á–∞–Ω–∏–∫',
    70000: '–ò–∑–≤–µ—Å—Ç–Ω—è–∫',
    80000: '–ú–µ—Ä–≥–µ–ª—å',
    88000: '–ò–∑–≤–µ—Å—Ç–∫–æ–≤–∏—Å—Ç–∞—è –≥–ª–∏–Ω–∞',
    90000: '–î–æ–ª–æ–º–∏—Ç',
    74000: '–ú–µ–ª',
    86000: '–ì–ª–∏–Ω–∞',
    93000: '–¢—É—Ñ',
    70032: '–ü–µ—Å—á–∞–Ω–∏—Å—Ç—ã–π –∏–∑–≤–µ—Å—Ç–Ω—è–∫',
    99000: '–ê–Ω–≥–∏–¥—Ä–∏—Ç'
}

df['–õ–ò–¢–û–õ–û–ì–ò–Ø_–¢–ï–ö–°–¢'] = df['–õ–ò–¢–û–õ–û–ì–ò–Ø'].map(lithology_dict)

print("\nüéØ –ê–ù–ê–õ–ò–ó –õ–ò–¢–û–õ–û–ì–ò–ò")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {df['–õ–ò–¢–û–õ–û–ì–ò–Ø_–¢–ï–ö–°–¢'].nunique()}")
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
lithology_counts = df['–õ–ò–¢–û–õ–û–ì–ò–Ø_–¢–ï–ö–°–¢'].value_counts()
print(lithology_counts)

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ª–∏—Ç–æ–ª–æ–≥–∏–∏
plt.figure(figsize=(12, 6))
lithology_counts.plot(kind='bar', color='skyblue')
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–∏–ø–æ–≤ –ª–∏—Ç–æ–ª–æ–≥–∏–∏')
plt.xlabel('–¢–∏–ø –ª–∏—Ç–æ–ª–æ–≥–∏–∏')
plt.ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—Ä–∞–∑—Ü–æ–≤')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('images/lithology_distribution.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úÖ –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª–∏—Ç–æ–ª–æ–≥–∏–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ images/lithology_distribution.png")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ª–∏—Ç–æ–ª–æ–≥–∏–∏
lithology_counts.to_csv('results/lithology_statistics.csv', encoding='utf-8-sig')

# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤
lithology_merge = {
    65030: 30000,  # –ì–ª–∏–Ω–∏—Å—Ç—ã–π –ø–µ—Å—á–∞–Ω–∏–∫ ‚Üí –ü–µ—Å—á–∞–Ω–∏–∫-–ì–ª–∏–Ω–∞
    86000: 88000,  # –ì–ª–∏–Ω–∞ ‚Üí –ò–∑–≤–µ—Å—Ç–∫–æ–≤–∏—Å—Ç–∞—è –≥–ª–∏–Ω–∞
    74000: 70000,  # –ú–µ–ª ‚Üí –ò–∑–≤–µ—Å—Ç–Ω—è–∫
    70032: 70000,  # –ü–µ—Å—á–∞–Ω–∏—Å—Ç—ã–π –∏–∑–≤–µ—Å—Ç–Ω—è–∫ ‚Üí –ò–∑–≤–µ—Å—Ç–Ω—è–∫
    93000: 70000,  # –¢—É—Ñ ‚Üí –ò–∑–≤–µ—Å—Ç–Ω—è–∫
}

# –†–∞–±–æ—Ç–∞–µ–º —Å –ø–æ–¥–≤—ã–±–æ—Ä–∫–æ–π –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
df_sample = df.sample(frac=0.1, random_state=42)
df_sample['–õ–ò–¢–û–õ–û–ì–ò–Ø'] = df_sample['–õ–ò–¢–û–õ–û–ì–ò–Ø'].replace(lithology_merge)

print(f"\nüìä –†–∞–∑–º–µ—Ä –ø–æ–¥–≤—ã–±–æ—Ä–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {df_sample.shape}")

# =============================================================================
# FEATURE ENGINEERING - –°–û–ó–î–ê–ù–ò–ï –ù–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í
# =============================================================================
print("\nüîß –°–û–ó–î–ê–ù–ò–ï –ù–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í...")

# –ì–µ–æ—Ñ–∏–∑–∏—á–µ—Å–∫–∏–µ –æ—Ç–Ω–æ—à–µ–Ω–∏—è
df_sample['–ì–ö/–ü–õ–û–¢–ù–û–°–¢–¨'] = df_sample['–ì–ö'] / df_sample['–ü–õ–û–¢–ù–û–°–¢–¨']
df_sample['–ù–ù–ö*–ü–õ–û–¢–ù–û–°–¢–¨'] = df_sample['–ù–ù–ö'] * df_sample['–ü–õ–û–¢–ù–û–°–¢–¨']
df_sample['RES_–ì–õ–£–ë–û–ö–ê–Ø/RES_–°–†–ï–î–ù–Ø–Ø'] = df_sample['RES_–ì–õ–£–ë–û–ö–ê–Ø'] / df_sample['RES_–°–†–ï–î–ù–Ø–Ø']

# –°–∫–æ–ª—å–∑—è—â–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
df_sample['–ì–ö_–ú–ê'] = df_sample.groupby('–°–ö–í–ê–ñ–ò–ù–ê')['–ì–ö'].transform(
    lambda x: x.rolling(window=5, min_periods=1).mean()
)
df_sample['–ü–õ–û–¢–ù–û–°–¢–¨_STD'] = df_sample.groupby('–°–ö–í–ê–ñ–ò–ù–ê')['–ü–õ–û–¢–ù–û–°–¢–¨'].transform(
    lambda x: x.rolling(window=5, min_periods=1).std()
)

# –†–∞–∑–Ω–æ—Å—Ç–∏
df_sample['–ì–ö_DIFF'] = df_sample.groupby('–°–ö–í–ê–ñ–ò–ù–ê')['–ì–ö'].diff()
df_sample['–ü–õ–û–¢–ù–û–°–¢–¨_DIFF'] = df_sample.groupby('–°–ö–í–ê–ñ–ò–ù–ê')['–ü–õ–û–¢–ù–û–°–¢–¨'].diff()

# –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –≤ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö
new_features = ['–ì–ö/–ü–õ–û–¢–ù–û–°–¢–¨', '–ù–ù–ö*–ü–õ–û–¢–ù–û–°–¢–¨', 'RES_–ì–õ–£–ë–û–ö–ê–Ø/RES_–°–†–ï–î–ù–Ø–Ø',
                '–ì–ö_DIFF', '–ü–õ–û–¢–ù–û–°–¢–¨_DIFF', '–ì–ö_–ú–ê', '–ü–õ–û–¢–ù–û–°–¢–¨_STD']

for feature in new_features:
    df_sample[feature] = df_sample[feature].fillna(df_sample[feature].median())

print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(new_features)} –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")

# =============================================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø –ú–û–î–ï–õ–ò
# =============================================================================
# –£–¥–∞–ª—è–µ–º –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∏ —Ü–µ–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
columns_to_drop = ['–õ–ò–¢–û–õ–û–ì–ò–Ø', '–õ–ò–¢–û–õ–û–ì–ò–Ø_–¢–ï–ö–°–¢', '–°–ö–í–ê–ñ–ò–ù–ê', '–ì–†–£–ü–ü–ê',
                   "–§–û–†–ú–ê–¶–ò–Ø", '–ö–ê–õ–ò–ë–†_–†–ê–°–•–û–ñ–î–ï–ù–ò–ï', '–ü–õ–û–¢–ù–û–°–¢–¨_–†–ê–°–¢–í–û–†–ê',
                   '–°–ö–û–†–û–°–¢–¨_–ü–†–û–•–û–î–ö–ò', 'RES_–ó–û–ù–ê_–ü–†–û–ù–ò–ö–ù–û–í–ï–ù–ò–Ø']

# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∫–∞–∫–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
existing_columns_to_drop = [col for col in columns_to_drop if col in df_sample.columns]
X = df_sample.drop(existing_columns_to_drop, axis=1)
y = df_sample['–õ–ò–¢–û–õ–û–ì–ò–Ø']

print(f"‚úÖ –ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {X.shape[1]}")
print(f"‚úÖ –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è: {y.nunique()} –∫–ª–∞—Å—Å–æ–≤")

# –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
le = LabelEncoder()
y_encoded = le.fit_transform(y)

print("\nüìù –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï –ö–õ–ê–°–°–û–í:")
for i, class_name in enumerate(le.classes_):
    print(f"{i:2d} -> {class_name:6} ({lithology_dict[class_name]})")

# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
X_train, X_test, y_train, y_test = train_test_split(
    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

print(f"\nüìä –†–ê–ó–ë–ò–ï–ù–ò–ï –î–ê–ù–ù–´–•:")
print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")

# =============================================================================
# –ú–û–î–ï–õ–¨ XGBOOST –° –ù–û–í–´–ú–ò –ü–†–ò–ó–ù–ê–ö–ê–ú–ò
# =============================================================================
print("\nüöÄ –û–ë–£–ß–ï–ù–ò–ï XGBOOST –° FEATURE ENGINEERING...")

xgb_model = XGBClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=6,
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train, y_train)

# –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏
y_pred = xgb_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average='macro')

print(f"‚úÖ Accuracy: {accuracy:.3f}")
print(f"‚úÖ F1-score (macro): {f1:.3f}")

print("\nüìà –î–ï–¢–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢:")
print(classification_report(y_test, y_pred,
                          target_names=[lithology_dict[le.classes_[i]] for i in range(len(le.classes_))]))

# =============================================================================
# –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í
# =============================================================================
importances = pd.DataFrame({
    'feature': X.columns,
    'importance': xgb_model.feature_importances_
}).sort_values('importance', ascending=False)

print("\nüîù –¢–û–ü-10 –°–ê–ú–´–• –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
print(importances.head(10))

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
importances.to_csv('results/feature_importance.csv', encoding='utf-8-sig', index=False)
print("‚úÖ –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ results/feature_importance.csv")

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
plt.figure(figsize=(10, 8))
top_features = importances.head(15)
sns.barplot(data=top_features, y='feature', x='importance', palette='viridis')
plt.title('–¢–æ–ø-15 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')
plt.xlabel('–í–∞–∂–Ω–æ—Å—Ç—å')
plt.tight_layout()
plt.savefig('images/feature_importance.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úÖ –ì—Ä–∞—Ñ–∏–∫ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ images/feature_importance.png")

# –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤—ã—Ö —Ñ–∏—á–µ–π
print(f"\nüéØ –†–ï–ô–¢–ò–ù–ì –ù–û–í–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
new_features_rank = []
for feature in new_features:
    if feature in importances['feature'].values:
        rank = importances[importances['feature'] == feature].index[0] + 1
        imp = importances[importances['feature'] == feature]['importance'].values[0]
        new_features_rank.append((rank, feature, imp))
        print(f"{rank:2d}. {feature:25} (–≤–∞–∂–Ω–æ—Å—Ç—å: {imp:.4f})")

# =============================================================================
# SHAP –ê–ù–ê–õ–ò–ó
# =============================================================================
print("\nüß† SHAP –ê–ù–ê–õ–ò–ó...")

try:
    # –ë–µ—Ä–µ–º –ø–æ–¥–≤—ã–±–æ—Ä–∫—É –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
    sample_idx = np.random.choice(len(X_test), size=min(500, len(X_test)), replace=False)
    X_sample = X_test.iloc[sample_idx]

    explainer = shap.TreeExplainer(xgb_model)
    shap_values = explainer.shap_values(X_sample)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    plt.figure(figsize=(10, 8))
    shap.summary_plot(shap_values, X_sample, plot_type="bar", show=False)
    plt.title("SHAP Feature Importance")
    plt.tight_layout()
    plt.savefig('images/shap_feature_importance.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("‚úÖ SHAP –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ images/shap_feature_importance.png")

    # –î–µ—Ç–∞–ª—å–Ω—ã–π SHAP plot
    plt.figure(figsize=(12, 8))
    shap.summary_plot(shap_values, X_sample, show=False)
    plt.tight_layout()
    plt.savefig('images/shap_summary_plot.png', dpi=300, bbox_inches='tight')
    plt.show()
    print("‚úÖ –î–µ—Ç–∞–ª—å–Ω—ã–π SHAP plot —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ images/shap_summary_plot.png")

except Exception as e:
    print(f"‚ö†Ô∏è SHAP –∞–Ω–∞–ª–∏–∑ –Ω–µ —É–¥–∞–ª—Å—è: {e}")
    print("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ SHAP...")

# =============================================================================
# –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò
# =============================================================================
print("\n‚ö° –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ú–û–î–ï–õ–ò...")

# –£–¥–∞–ª—è–µ–º —Å–ª–∞–±—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤–∞–∂–Ω–æ—Å—Ç—å < 0.01)
features_to_drop = []
for feature in new_features:
    if feature in importances['feature'].values:
        imp = importances[importances['feature'] == feature]['importance'].values[0]
        if imp < 0.01:
            features_to_drop.append(feature)

if features_to_drop:
    print(f"üóëÔ∏è –£–¥–∞–ª—è–µ–º —Å–ª–∞–±—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {features_to_drop}")
    X_clean = X.drop(features_to_drop, axis=1)
else:
    print("‚úÖ –ù–µ—Ç —Å–ª–∞–±—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è")
    X_clean = X.copy()

# –ü–µ—Ä–µ—Ä–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
X_train_clean, X_test_clean, y_train_clean, y_test_clean = train_test_split(
    X_clean, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
)

# –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å
xgb_final = XGBClassifier(
    n_estimators=150,
    learning_rate=0.05,
    max_depth=8,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

xgb_final.fit(X_train_clean, y_train_clean)

# –§–∏–Ω–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
y_pred_final = xgb_final.predict(X_test_clean)
final_accuracy = accuracy_score(y_test_clean, y_pred_final)
final_f1 = f1_score(y_test_clean, y_pred_final, average='macro')

print(f"üéØ –§–ò–ù–ê–õ–¨–ù–´–ô ACCURACY: {final_accuracy:.3f}")
print(f"üéØ –§–ò–ù–ê–õ–¨–ù–´–ô F1-SCORE: {final_f1:.3f}")

# –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
cm = confusion_matrix(y_test_clean, y_pred_final)
plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=[lithology_dict[cls] for cls in le.classes_],
            yticklabels=[lithology_dict[cls] for cls in le.classes_])
plt.title('–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ - –§–∏–Ω–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å')
plt.xlabel('–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
plt.ylabel('–ò—Å—Ç–∏–Ω–Ω—ã–π –∫–ª–∞—Å—Å')
plt.xticks(rotation=45)
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('images/confusion_matrix.png', dpi=300, bbox_inches='tight')
plt.show()
print("‚úÖ –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ images/confusion_matrix.png")

# =============================================================================
# –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ò –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
# =============================================================================
print("\n" + "="*60)
print("üéâ –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –ü–†–û–ï–ö–¢–ê")
print("="*60)

final_importances = pd.DataFrame({
    'feature': X_clean.columns,
    'importance': xgb_final.feature_importances_
}).sort_values('importance', ascending=False)

print(f"–§–∏–Ω–∞–ª—å–Ω—ã–π accuracy (–æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è): {final_accuracy:.3f}")
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(X_clean.columns)}")

print("\nüîù –¢–û–ü-5 –°–ê–ú–´–• –í–ê–ñ–ù–´–• –ü–†–ò–ó–ù–ê–ö–û–í:")
print(final_importances.head(5))

# –ê–Ω–∞–ª–∏–∑ –Ω–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤ —Ç–æ–ø–µ
new_features_in_top = []
for feature in ['–ì–ö/–ü–õ–û–¢–ù–û–°–¢–¨', '–ù–ù–ö*–ü–õ–û–¢–ù–û–°–¢–¨', '–ì–ö_–ú–ê', '–ü–õ–û–¢–ù–û–°–¢–¨_STD']:
    if feature in final_importances['feature'].values:
        rank = final_importances[final_importances['feature'] == feature].index[0] + 1
        imp = final_importances[final_importances['feature'] == feature]['importance'].values[0]
        new_features_in_top.append((feature, rank, imp))

print(f"\nüéØ –ù–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –≤ —Ç–æ–ø–µ: {len(new_features_in_top)}")
for feature, rank, imp in new_features_in_top:
    print(f"   {rank}. {feature} (–≤–∞–∂–Ω–æ—Å—Ç—å: {imp:.4f})")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
results_summary = {
    'final_accuracy': final_accuracy,
    'final_f1_score': final_f1,
    'num_features': len(X_clean.columns),
    'top_features': final_importances.head(10)['feature'].tolist()
}

import json
with open('results/final_results.json', 'w', encoding='utf-8') as f:
    json.dump(results_summary, f, ensure_ascii=False, indent=2)

print("\nüíæ –í–°–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –°–û–•–†–ê–ù–ï–ù–´:")
print("   üìä images/ - –≥—Ä–∞—Ñ–∏–∫–∏ –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏")
print("   üìà results/ - –º–µ—Ç—Ä–∏–∫–∏ –∏ –∞–Ω–∞–ª–∏–∑")
print("   ‚úÖ requirements.txt - –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –ø—Ä–æ–µ–∫—Ç–∞")

print("\n‚úÖ –ü–†–û–ï–ö–¢ –£–°–ü–ï–®–ù–û –ó–ê–í–ï–†–®–ï–ù!")
